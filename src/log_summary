------------------------------------------------------------
Execute EigenProblem<3>
[timer] h = 0.15
 number of elements : 7254
 number of faces : 15917
 number of edges : 0
 number of points : 1961
 number of vertices : 2961
 number of local vertices : 4
[timer] mesh = 2.35107 sec
[timer] form = 4.94321 sec
[timer] info = 0.03651 sec
[timer] matrixC = 0.172298 sec
[timer] hat = 0.205689 sec
nev = 10	 ncv= 50
[timer] 6 eigs = 8.49355 sec
eigenvalue 0 = (20.0126,0)
eigenvalue 1 = (20.042,0)
eigenvalue 2 = (20.0439,0)
eigenvalue 3 = (20.3592,0)
eigenvalue 4 = (20.3776,0)
eigenvalue 5 = (20.3877,0)
[timer] export = 1.79791 sec
[timer] total = 18.0055 sec
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------

feelpp_po_basischange on a linux-gnu-c-opt named irma-atlas with 5 processors, by hild Wed Feb 11 12:44:52 2015
Using Petsc Release Version 3.4.2, Jul, 02, 2013 

                         Max       Max/Min        Avg      Total 
Time (sec):           1.812e+01      1.00328   1.807e+01
Objects:              2.810e+02      1.00000   2.810e+02
Flops:                2.518e+09      1.34714   2.160e+09  1.080e+10
Flops/sec:            1.394e+08      1.34699   1.195e+08  5.975e+08
MPI Messages:         1.091e+04      1.00567   1.089e+04  5.443e+04
MPI Message Lengths:  5.604e+07      1.18328   4.652e+03  2.532e+08
MPI Reductions:       3.230e+03      1.00000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flops
                            and VecAXPY() for complex vectors of length N --> 8N flops

Summary of Stages:   ----- Time ------  ----- Flops -----  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total   counts   %Total     Avg         %Total   counts   %Total 
 0:      Main Stage: 1.8074e+01 100.0%  1.0799e+10 100.0%  5.443e+04 100.0%  4.652e+03      100.0%  3.229e+03 100.0% 

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flops: Max - maximum over all processors
                   Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   Avg. len: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %f - percent flops in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flops over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------
Event                Count      Time (sec)     Flops                             --- Global ---  --- Stage ---   Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   Avg len Reduct  %T %f %M %L %R  %T %f %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

UpdateVectors          1 1.0 1.4720e-03 1.3 9.12e+06 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0 27317
MatMult             2598 1.0 2.5457e+00 1.2 2.48e+09 1.3 5.2e+04 4.5e+03 0.0e+00 13 98 95 93  0  13 98 95 93  0  4171
MatMultAdd             6 1.0 8.3208e-04 1.2 3.82e+05 1.3 1.1e+02 1.3e+03 0.0e+00  0  0  0  0  0   0  0  0  0  0  2037
MatSolve              57 1.0 1.3949e+00 1.0 0.00e+00 0.0 1.2e+03 4.1e+03 6.1e+01  8  0  2  2  2   8  0  2  2  2     0
MatCholFctrSym         2 1.0 2.5983e-01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 1.4e+01  1  0  0  0  0   1  0  0  0  0     0
MatCholFctrNum         2 1.0 4.1200e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 2.0e+00 23  0  0  0  0  23  0  0  0  0     0
MatCopy                1 1.0 1.4241e-03 1.9 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatConvert             1 1.0 4.0910e-03 2.1 0.00e+00 0.0 0.0e+00 0.0e+00 1.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyBegin      28 1.0 3.6162e-0233.1 0.00e+00 0.0 8.1e+01 6.5e+03 4.6e+01  0  0  0  0  1   0  0  0  0  1     0
MatAssemblyEnd        28 1.0 2.8073e-02 1.1 0.00e+00 0.0 3.1e+02 7.6e+02 6.4e+01  0  0  1  0  2   0  0  1  0  2     0
MatGetRow           7300 1.3 9.2919e-03 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetRowIJ            2 1.0 5.2452e-06 1.8 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetOrdering         2 1.0 6.6042e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatZeroEntries         5 1.0 1.7214e-04 1.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAXPY                2 1.0 8.6108e-02 1.0 0.00e+00 0.0 8.0e+01 1.1e+03 3.2e+01  0  0  0  0  1   0  0  0  0  1     0
MatPtAP                2 1.0 2.0360e-01 1.0 1.31e+07 1.3 3.3e+02 3.5e+04 5.0e+01  1  1  1  5  2   1  1  1  5  2   281
MatPtAPSymbolic        2 1.0 7.7409e-02 1.0 0.00e+00 0.0 2.2e+02 2.2e+04 3.0e+01  0  0  0  2  1   0  0  0  2  1     0
MatPtAPNumeric         2 1.0 1.2626e-01 1.0 1.31e+07 1.3 1.2e+02 6.1e+04 2.0e+01  1  1  0  3  1   1  1  0  3  1   453
MatGetLocalMat         2 1.0 7.5603e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetBrAoCol          2 1.0 1.1358e-03 1.1 0.00e+00 0.0 1.1e+02 1.1e+04 4.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatGetSymTrans         4 1.0 4.2605e-04 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecNorm               12 1.0 1.3313e-03 7.3 4.38e+04 1.3 0.0e+00 0.0e+00 1.2e+01  0  0  0  0  0   0  0  0  0  0   145
VecScale              56 1.0 3.6168e-04 1.6 1.02e+05 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  1245
VecCopy               12 1.0 5.5790e-05 1.4 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecSet                98 1.0 4.6945e-04 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAXPY             2478 1.0 1.1446e-02 1.6 9.04e+06 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  3482
VecAssemblyBegin      30 1.0 1.3316e-03 1.5 0.00e+00 0.0 0.0e+00 0.0e+00 9.0e+01  0  0  0  0  3   0  0  0  0  3     0
VecAssemblyEnd        30 1.0 4.1723e-05 1.3 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecScatterBegin     2754 1.0 7.8105e-02 1.2 0.00e+00 0.0 5.4e+04 4.5e+03 5.7e+01  0  0 98 95  2   0  0 98 95  2     0
VecScatterEnd       2697 1.0 4.5239e-01 4.6 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
VecReduceArith      2625 1.0 8.0156e-03 1.2 9.58e+06 1.3 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0  5266
VecReduceComm       2625 1.0 3.6362e-0110.2 0.00e+00 0.0 0.0e+00 0.0e+00 2.6e+03  1  0  0  0 81   1  0  0  0 81     0
EPSSetUp               1 1.0 2.2422e+00 1.0 0.00e+00 0.0 4.0e+01 1.1e+03 1.4e+02 12  0  0  0  4  12  0  0  0  4     0
EPSSolve               1 1.0 8.4668e+00 1.0 2.49e+09 1.3 5.3e+04 4.5e+03 2.9e+03 47 99 97 95 89  47 99 97 95 89  1263
IPOrthogonalize       51 1.0 2.5298e+00 1.0 2.42e+09 1.3 5.0e+04 4.5e+03 2.6e+03 14 96 93 90 81  14 96 93 90 81  4108
IPInnerProduct      2778 1.0 2.5280e+00 1.0 2.42e+09 1.3 5.1e+04 4.5e+03 2.6e+03 14 96 93 91 81  14 96 93 91 81  4105
IPApplyMatrix       2529 1.0 2.4775e+00 1.2 2.41e+09 1.3 5.1e+04 4.5e+03 0.0e+00 13 96 93 91  0  13 96 93 91  0  4172
STSetUp                1 1.0 2.2406e+00 1.0 0.00e+00 0.0 4.0e+01 1.1e+03 3.8e+01 12  0  0  0  1  12  0  0  0  1     0
STApply               57 1.0 1.4476e+00 1.0 5.43e+07 1.3 2.3e+03 4.3e+03 6.1e+01  8  2  4  4  2   8  2  4  4  2   161
DSSolve                1 1.0 1.1520e-03 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DSVectors             50 1.0 4.2200e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
DSOther                1 1.0 3.5048e-05 1.1 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               2 1.0 2.6941e-05 1.2 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve              57 1.0 1.3962e+00 1.0 0.00e+00 0.0 1.2e+03 4.1e+03 6.1e+01  8  0  2  2  2   8  0  2  2  2     0
PCSetUp                2 1.0 4.3824e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 3.8e+01 24  0  0  0  1  24  0  0  0  1     0
PCApply               57 1.0 1.3950e+00 1.0 0.00e+00 0.0 1.2e+03 4.1e+03 6.1e+01  8  0  2  2  2   8  0  2  2  2     0
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

           Container     2              2         1144     0
              Matrix    38             30     21418264     0
              Vector   128            125      1244568     0
      Vector Scatter    24             22        22808     0
           Index Set    67             67       339632     0
   IS L to G Mapping    14             14       127552     0
Eigenvalue Problem Solver     1              1        25584     0
         PetscRandom     1              1          632     0
       Inner product     1              1          664     0
  Spectral Transform     1              1          816     0
       Direct solver     1              1        66012     0
       Krylov Solver     1              1         1152     0
      Preconditioner     1              1         1056     0
              Viewer     1              0            0     0
========================================================================================================================
Average time to get PetscTime(): 9.53674e-08
Average time for MPI_Barrier(): 1.00136e-05
Average time for zero size MPI_Send(): 9.77516e-06
#PETSc Option Table entries:
--gmsh.filename sphere.geo
--gmsh.hsize 0.15
--solvereigen.ncv 50
--solvereigen.nev 10
--solvereigen.solver krylovschur
--useSphere 1
-eps_interval 19,21
-log_summary
-mat_mumps_icntl_13 1
-st_ksp_type preonly
-st_pc_factor_mat_solver_package mumps
-st_pc_type cholesky
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure run at: Wed Aug 27 14:19:17 2014
Configure options: --with-shared-libraries --with-debugging=0 --useThreads 0 --with-clanguage=C++ --with-c-support --with-fortran-interfaces=1 --with-mpi-dir=/usr/lib/openmpi --with-mpi-shared=1 --with-blas-lib=-lblas --with-lapack-lib=-llapack --with-blacs=1 --with-blacs-include=/usr/include --with-blacs-lib="[/usr/lib/libblacsCinit-openmpi.so,/usr/lib/libblacs-openmpi.so]" --with-scalapack=1 --with-scalapack-include=/usr/include --with-scalapack-lib=/usr/lib/libscalapack-openmpi.so --with-mumps=1 --with-mumps-include=/usr/include --with-mumps-lib="[/usr/lib/libdmumps.so,/usr/lib/libzmumps.so,/usr/lib/libsmumps.so,/usr/lib/libcmumps.so,/usr/lib/libmumps_common.so,/usr/lib/libpord.so]" --with-umfpack=1 --with-umfpack-include=/usr/include/suitesparse --with-umfpack-lib="[/usr/lib/libumfpack.so,/usr/lib/libamd.so]" --with-cholmod=1 --with-cholmod-include=/usr/include/suitesparse --with-cholmod-lib=/usr/lib/libcholmod.so --with-spooles=1 --with-spooles-include=/usr/include/spooles --with-spooles-lib=/usr/lib/libspooles.so --with-hypre=1 --with-hypre-dir=/usr --with-ptscotch=1 --with-ptscotch-include=/usr/include/scotch --with-ptscotch-lib="[/usr/lib/libptesmumps.so,/usr/lib/libptscotch.so,/usr/lib/libptscotcherr.so]" --with-fftw=1 --with-fftw-include=/usr/include --with-fftw-lib="[/usr/lib/x86_64-linux-gnu/libfftw3.so,/usr/lib/x86_64-linux-gnu/libfftw3_mpi.so]" --with-hdf5=1 --with-hdf5-dir=/usr/lib/x86_64-linux-gnu/hdf5/openmpi --CXX_LINKER_FLAGS=-Wl,--no-as-needed
-----------------------------------------
Libraries compiled on Wed Aug 27 14:19:17 2014 on binet 
Machine characteristics: Linux-3.2.0-4-amd64-x86_64-with-debian-jessie-sid
Using PETSc directory: /build/petsc-lccVo9/petsc-3.4.2.dfsg1
Using PETSc arch: linux-gnu-c-opt
-----------------------------------------

Using C compiler: mpicxx  -Wall -Wwrite-strings -Wno-strict-aliasing -Wno-unknown-pragmas -O   -fPIC   ${COPTFLAGS} ${CFLAGS}
Using Fortran compiler: mpif90  -fPIC -Wall -Wno-unused-variable -Wno-unused-dummy-argument -O   ${FOPTFLAGS} ${FFLAGS} 
-----------------------------------------

Using include paths: -I/build/petsc-lccVo9/petsc-3.4.2.dfsg1/linux-gnu-c-opt/include -I/build/petsc-lccVo9/petsc-3.4.2.dfsg1/include -I/build/petsc-lccVo9/petsc-3.4.2.dfsg1/include -I/build/petsc-lccVo9/petsc-3.4.2.dfsg1/linux-gnu-c-opt/include -I/usr/include -I/usr/include/suitesparse -I/usr/include/scotch -I/usr/lib/x86_64-linux-gnu/hdf5/openmpi/include -I/usr/lib/openmpi/include -I/usr/lib/openmpi/include/openmpi
-----------------------------------------

Using C linker: mpicxx
Using Fortran linker: mpif90
Using libraries: -L/build/petsc-lccVo9/petsc-3.4.2.dfsg1/linux-gnu-c-opt/lib -L/build/petsc-lccVo9/petsc-3.4.2.dfsg1/linux-gnu-c-opt/lib -lpetsc -L/usr/lib -lHYPRE_utilities -lHYPRE_struct_mv -lHYPRE_struct_ls -lHYPRE_sstruct_mv -lHYPRE_sstruct_ls -lHYPRE_IJ_mv -lHYPRE_parcsr_ls -ldmumps -lzmumps -lsmumps -lcmumps -lmumps_common -lpord -lscalapack-openmpi -lcholmod -lumfpack -lamd -llapack -lblas -lX11 -lpthread -lptesmumps -lptscotch -lptscotcherr -L/usr/lib/x86_64-linux-gnu -lfftw3 -lfftw3_mpi -L/usr/lib/x86_64-linux-gnu/hdf5/openmpi/lib -lhdf5_fortran -lhdf5_hl -lhdf5 -lz -lm -L/usr/lib/openmpi/lib -L/usr/lib/gcc/x86_64-linux-gnu/4.9.1 -L/lib/x86_64-linux-gnu -lmpi_f90 -lmpi_f77 -lgfortran -lm -lgfortran -lm -lquadmath -lm -lmpi_cxx -lstdc++ -ldl -lmpi -lhwloc -lgcc_s -lpthread -ldl 
-----------------------------------------

